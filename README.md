# Отчет о проекте по веб-скрапингу

## Выполненные шаги
1. Создана БД PostgreSQL и модели данных для хранения скрапленных данных об аукционах
2. Написан парсер на Python с использованием Requests и lxml для скрапинга страниц с аукционами с сайта https://nedradv.ru/
3. Парсер извлекает ключевые данные с каждой страницы: дату, место, статус и т.д.
4. Далее переходит по ссылкам на страницы с деталями аукциона для получения дедлайна, платы за участие, организатора
5. Проходит по всем страницам пагинации, парсит данные и сохраняет в БД
6. Созданы таблицы и связи между данными с помощью ORM SQLAlchemy

## Возникшие проблемы
* Пагинация на сайте была некорректной, пришлось обработать пропуски в номерах страниц
* Периодически возникали таймауты и ошибки при слишком частых запросах, пришлось добавить задержки
* На некоторых страницах отсутствовали данные по отдельным полям, например дедлайн
* Потребовалась проверка на пустую строку перед записью в БД
* Первоначальная схема БД потребовала рефакторинга после анализа доступных данных
* Настройка подключения к БД PostgreSQL потребовала нескольких итераций
* В целом проект прошел довольно гладко, но обработка реальных данных с рабочего сайта выявила ряд нюансов. После доработки парсера и обработки угловых случаев, удалось извлечь большую часть доступных данных.